{"cells": [{"cell_type": "markdown", "id": "b42da0ab", "metadata": {}, "source": ["# Building a Regression Model in PyTorch"]}, {"cell_type": "markdown", "id": "43e8baa2", "metadata": {}, "source": ["PyTorch library is for deep learning. \n", "Some applications of deep learning models are to solve regression or classification problems.\n", "In this tutorial, you will discover how to use PyTorch to develop and evaluate neural network models for regression problems."]}, {"cell_type": "markdown", "id": "75f0e761", "metadata": {}, "source": ["## Outcome\n", "\n", "After completing this tutorial, you will know:\n", "- How to load data from scikit-learn and adapt it for PyTorch models\n", "- How to create a neural network for regerssion problem using PyTorch\n", "- How to improve model performance with data preparation techniques"]}, {"cell_type": "markdown", "id": "44583e2a", "metadata": {}, "source": ["## Description of the Dataset"]}, {"cell_type": "markdown", "id": "f3803c22", "metadata": {}, "source": ["The dataset you will use in this tutorial is the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n", "\n", "This is a dataset that describes the median house value for California districts. \n", "Each data sample is a census block group. \n", "The target variable is the median house value in USD 100,000 in 1990 and there are 8 input features, each describing something about the house. \n", "They are, namely,\n", "\n", "- MedInc: median income in block group\n", "- HouseAge: median house age in block group\n", "- AveRooms: average number of rooms per household\n", "- AveBedrms: average number of bedrooms per household\n", "- Population: block group population\n", "- AveOccup: average number of household members\n", "- Latitude: block group centroid latitude\n", "- Longitude: block group centroid longitude\n", "\n", "This data is special because the input data is in vastly different scale. \n", "For example, the number of rooms per house is usually small but the population per block group is usually large. \n", "Moreover, most features should be positive but the longitude must be negative (because that's about California). \n", "Handling such diversity of data is a challenge to some machine learning models."]}, {"cell_type": "markdown", "id": "f4a81276", "metadata": {}, "source": ["You can get the dataset from scikit-learn, which in turn, is downloaded from the Internet at realtime:"]}, {"cell_type": "code", "execution_count": 2, "id": "ee62407a", "metadata": {"lines_to_next_cell": 0}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"]}], "source": ["from sklearn.datasets import fetch_california_housing\n", " \n", "data = fetch_california_housing()\n", "print(data.feature_names)\n", " \n", "X, y = data.data, data.target"]}, {"cell_type": "markdown", "id": "ad8ad182", "metadata": {}, "source": ["## Building a Model and Train"]}, {"cell_type": "markdown", "id": "2f01ae23", "metadata": {}, "source": ["This is a regression problem. \n", "Unlike classification problems, the output variable is a continuous value. \n", "In case of neural networks, you usually use linear activation at the output layer (i.e., no activation) such that the output range theoretically can be anything from negative infinty to positive infinity.\n", "\n", "Also for regression problems, you should never expect the model to predict the values perfectly. \n", "Therefore, you should care about how close the prediction is to the actual value. \n", "The loss metric that you can use for this is the mean square error (MSE) or mean absolute error (MAE). \n", "But you may also interested in the root mean squared error (RMSE) because that's a metric in the same unit as your output variable.\n", "\n", "Let's try the traditional design of a neural network, namely, the pyramid structure. \n", "A pyramid structure is to have the number of neurons in each layer decreasing as the network progresses to the output. \n", "The number of input features is fixed, but you set a large number of neurons on the first hidden layer and gradually reduce the number in the subsequent layers. \n", "Because you have only one target in this dataset, the final layer should output only one value."]}, {"cell_type": "markdown", "id": "13549df8", "metadata": {}, "source": ["One design is as follows:"]}, {"cell_type": "code", "execution_count": 3, "id": "f2fc9fc3", "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", " \n", "# Define the model\n", "model = nn.Sequential(\n", "    nn.Linear(8, 24),\n", "    nn.ReLU(),\n", "    nn.Linear(24, 12),\n", "    nn.ReLU(),\n", "    nn.Linear(12, 6),\n", "    nn.ReLU(),\n", "    nn.Linear(6, 1)\n", ")"]}, {"cell_type": "markdown", "id": "c50804de", "metadata": {}, "source": ["To train this network, you need to define a loss function. \n", "MSE is a reasonable choice. \n", "You also need an optimizer, such as Adam."]}, {"cell_type": "code", "execution_count": 4, "id": "801b7520", "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "import torch.optim as optim\n", " \n", "# loss function and optimizer\n", "loss_fn = nn.MSELoss()  # mean square error\n", "optimizer = optim.Adam(model.parameters(), lr=0.0001)"]}, {"cell_type": "markdown", "id": "460f0c03", "metadata": {}, "source": ["To train this model, you can use your usual training loop. \n", "In order to obtain an evaluation score so you are confident that the model works, you need to split the data into training and test sets. \n", "You may also want to avoid overfitting by keeping track on the test set MSE. \n", "The following is the training loop with the train-test split:"]}, {"cell_type": "code", "execution_count": 5, "id": "df788618", "metadata": {}, "outputs": [{"data": {"text/plain": ["<All keys matched successfully>"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["import copy\n", "import numpy as np\n", "import torch\n", "import tqdm\n", "from sklearn.model_selection import train_test_split\n", " \n", "# train-test split of the dataset\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n", "X_train = torch.tensor(X_train, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n", "X_test = torch.tensor(X_test, dtype=torch.float32)\n", "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n", " \n", "# training parameters\n", "n_epochs = 100   # number of epochs to run\n", "batch_size = 10  # size of each batch\n", "batch_start = torch.arange(0, len(X_train), batch_size)\n", " \n", "# Hold the best model\n", "best_mse = np.inf   # init to infinity\n", "best_weights = None\n", "history = []\n", " \n", "# training loop\n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n", "        bar.set_description(f\"Epoch {epoch}\")\n", "        for start in bar:\n", "            # take a batch\n", "            X_batch = X_train[start:start+batch_size]\n", "            y_batch = y_train[start:start+batch_size]\n", "            # forward pass\n", "            y_pred = model(X_batch)\n", "            loss = loss_fn(y_pred, y_batch)\n", "            # backward pass\n", "            optimizer.zero_grad()\n", "            loss.backward()\n", "            # update weights\n", "            optimizer.step()\n", "            # print progress\n", "            bar.set_postfix(mse=float(loss))\n", "    # evaluate accuracy at end of each epoch\n", "    model.eval()\n", "    y_pred = model(X_test)\n", "    mse = loss_fn(y_pred, y_test)\n", "    mse = float(mse)\n", "    history.append(mse)\n", "    if mse < best_mse:\n", "        best_mse = mse\n", "        best_weights = copy.deepcopy(model.state_dict())\n", "\n", "# restore model and return best accuracy\n", "model.load_state_dict(best_weights)"]}, {"cell_type": "markdown", "id": "f533c6ac", "metadata": {}, "source": ["In the training loop, tqdm is used to set up a progress bar and in each iteration step, MSE is calculated and reported. \n", "You can see how the MSE changed by setting the tqdm parameter disable above to False.\n", "\n", "Note that in the training loop, each epoch is to run the forward and backward steps with the training set a few times to optimize the model weights, and at the end of the epoch, the model is evaluated using the test set. \n", "It is the MSE from the test set that is remembered in the list history. \n", "It is also the metric to evaluate a model, which the best one is stored in the variable best_weights.\n", "\n", "After you run this, you will have the best model restored and the best MSE stored in the variable best_mse. \n", "Note that the mean square error is the average of the square of the difference between the predicted value and the actual value. \n", "The square root of it, RMSE, can be regarded as the average difference and it is numerically more useful."]}, {"cell_type": "markdown", "id": "9f00f15f", "metadata": {}, "source": ["In below, you can show the MSE and RMSE, and plot the history of MSE. \n", "It should be decreasing with the epochs."]}, {"cell_type": "code", "execution_count": 6, "id": "dff758ea", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["MSE: 0.53\n", "RMSE: 0.73\n"]}, {"ename": "NameError", "evalue": "name 'plt' is not defined", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24900\\4242603589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSE: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"]}], "source": ["print(\"MSE: %.2f\" % best_mse)\n", "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n", "plt.plot(history)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "12cfd074", "metadata": {}, "source": ["Putting everything together, the following is the complete code."]}, {"cell_type": "code", "execution_count": null, "id": "9b534182", "metadata": {"lines_to_next_cell": 0}, "outputs": [], "source": ["import copy\n", " \n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import tqdm\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import fetch_california_housing\n", " \n", "# Read data\n", "data = fetch_california_housing()\n", "X, y = data.data, data.target\n", " \n", "# train-test split for model evaluation\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n", " \n", "# Convert to 2D PyTorch tensors\n", "X_train = torch.tensor(X_train, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n", "X_test = torch.tensor(X_test, dtype=torch.float32)\n", "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n", " \n", "# Define the model\n", "model = nn.Sequential(\n", "    nn.Linear(8, 24),\n", "    nn.ReLU(),\n", "    nn.Linear(24, 12),\n", "    nn.ReLU(),\n", "    nn.Linear(12, 6),\n", "    nn.ReLU(),\n", "    nn.Linear(6, 1)\n", ")\n", " \n", "# loss function and optimizer\n", "loss_fn = nn.MSELoss()  # mean square error\n", "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n", " \n", "n_epochs = 100   # number of epochs to run\n", "batch_size = 10  # size of each batch\n", "batch_start = torch.arange(0, len(X_train), batch_size)\n", " \n", "# Hold the best model\n", "best_mse = np.inf   # init to infinity\n", "best_weights = None\n", "history = []\n", " \n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n", "        bar.set_description(f\"Epoch {epoch}\")\n", "        for start in bar:\n", "            # take a batch\n", "            X_batch = X_train[start:start+batch_size]\n", "            y_batch = y_train[start:start+batch_size]\n", "            # forward pass\n", "            y_pred = model(X_batch)\n", "            loss = loss_fn(y_pred, y_batch)\n", "            # backward pass\n", "            optimizer.zero_grad()\n", "            loss.backward()\n", "            # update weights\n", "            optimizer.step()\n", "            # print progress\n", "            bar.set_postfix(mse=float(loss))\n", "    # evaluate accuracy at end of each epoch\n", "    model.eval()\n", "    y_pred = model(X_test)\n", "    mse = loss_fn(y_pred, y_test)\n", "    mse = float(mse)\n", "    history.append(mse)\n", "    if mse < best_mse:\n", "        best_mse = mse\n", "        best_weights = copy.deepcopy(model.state_dict())\n", "\n", "# restore model and return best accuracy\n", "model.load_state_dict(best_weights)\n", "print(\"MSE: %.2f\" % best_mse)\n", "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n", "plt.plot(history)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "6c220e00", "metadata": {}, "source": ["\n"]}, {"cell_type": "markdown", "id": "2aa98d69", "metadata": {}, "source": ["## Improving the Model with Preprocessing"]}, {"cell_type": "markdown", "id": "431f3b47", "metadata": {}, "source": ["In the above, you see the RMSE is 0.68. \n", "Indeed, it is easy to improve the RMSE by polishing the data before training. \n", "The problem of this dataset is the diversity of the features: Some are with a narrow range and some are wide. \n", "And some are small but positive while some are very negative. \n", "This indeed is not very nice to most of the machine learning model.\n", "\n", "One way to improve this is to apply a standard scaler. \n", "It is to convert each feature into their standard score. \n", "In other words, for each feature $x$, you replace it with\n", "\n", "$$\n", "z = \\frac{x - \\bar{x}}{\\sigma_x}\n", "$$\n", "\n", "Where $\\bar{x}$ is the mean of $x$ and $\\sigma_x$ is the standard deviation. \n", "This way, every transformed feature is centered around 0 and in a narrow range that around 70% of the samples are between -1 to +1. \n", "This can help the machine learning model to converge."]}, {"cell_type": "markdown", "id": "c8f0a8af", "metadata": {}, "source": ["You can apply the standard scaler from scikit-learn. \n", "The following is how you should modify the data preparation part of the above code:"]}, {"cell_type": "code", "execution_count": null, "id": "7f90e2b3", "metadata": {}, "outputs": [], "source": ["import torch\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import fetch_california_housing\n", "from sklearn.preprocessing import StandardScaler\n", " \n", "# Read data\n", "data = fetch_california_housing()\n", "X, y = data.data, data.target\n", " \n", "# train-test split for model evaluation\n", "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n", " \n", "# Standardizing data\n", "scaler = StandardScaler()\n", "scaler.fit(X_train_raw)\n", "X_train = scaler.transform(X_train_raw)\n", "X_test = scaler.transform(X_test_raw)\n", " \n", "# Convert to 2D PyTorch tensors\n", "X_train = torch.tensor(X_train, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n", "X_test = torch.tensor(X_test, dtype=torch.float32)\n", "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"]}, {"cell_type": "markdown", "id": "29582477", "metadata": {}, "source": ["Note that standard scaler is applied after train-test split. \n", "The StandardScaler above is fitted on the training set but applied on both the training and test set. \n", "You must not apply the standard scaler to all data because nothing from the test set should be hinted to the model. \n", "Otherwise you are introducing data leakage.\n", "\n", "Other than that, virtually nothing shall be changed: You still have 8 features (only they are not the same in value). \n", "You still use the same training loop. \n", "If you train the model with the scaled data, you should see the RMSE improved.\n", "\n", "While the MSE history is in a similar falling shape, the y-axis shows it is indeed better after scaling:\n", "\n", "However, you need to be careful at the end: When you use the trained model and apply to new data, you should apply the scaler to the input data before feed into the mode. \n", "That is, inference should be done as follows:"]}, {"cell_type": "code", "execution_count": null, "id": "904664d0", "metadata": {}, "outputs": [], "source": ["model.eval()\n", "with torch.no_grad():\n", "    # Test out inference with 5 samples from the original test set\n", "    for i in range(5):\n", "        X_sample = X_test_raw[i: i+1]\n", "        X_sample = scaler.transform(X_sample)\n", "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n", "        y_pred = model(X_sample)\n", "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"]}, {"cell_type": "markdown", "id": "4d76c940", "metadata": {}, "source": ["The following is the complete code:"]}, {"cell_type": "code", "execution_count": null, "id": "c3c954cf", "metadata": {}, "outputs": [], "source": ["import copy\n", " \n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import tqdm\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import fetch_california_housing\n", "from sklearn.preprocessing import StandardScaler\n", " \n", "# Read data\n", "data = fetch_california_housing()\n", "X, y = data.data, data.target\n", " \n", "# train-test split for model evaluation\n", "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n", " \n", "# Standardizing data\n", "scaler = StandardScaler()\n", "scaler.fit(X_train_raw)\n", "X_train = scaler.transform(X_train_raw)\n", "X_test = scaler.transform(X_test_raw)\n", " \n", "# Convert to 2D PyTorch tensors\n", "X_train = torch.tensor(X_train, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n", "X_test = torch.tensor(X_test, dtype=torch.float32)\n", "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n", " \n", "# Define the model\n", "model = nn.Sequential(\n", "    nn.Linear(8, 24),\n", "    nn.ReLU(),\n", "    nn.Linear(24, 12),\n", "    nn.ReLU(),\n", "    nn.Linear(12, 6),\n", "    nn.ReLU(),\n", "    nn.Linear(6, 1)\n", ")\n", " \n", "# loss function and optimizer\n", "loss_fn = nn.MSELoss()  # mean square error\n", "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n", " \n", "n_epochs = 100   # number of epochs to run\n", "batch_size = 10  # size of each batch\n", "batch_start = torch.arange(0, len(X_train), batch_size)\n", " \n", "# Hold the best model\n", "best_mse = np.inf   # init to infinity\n", "best_weights = None\n", "history = []\n", " \n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n", "        bar.set_description(f\"Epoch {epoch}\")\n", "        for start in bar:\n", "            # take a batch\n", "            X_batch = X_train[start:start+batch_size]\n", "            y_batch = y_train[start:start+batch_size]\n", "            # forward pass\n", "            y_pred = model(X_batch)\n", "            loss = loss_fn(y_pred, y_batch)\n", "            # backward pass\n", "            optimizer.zero_grad()\n", "            loss.backward()\n", "            # update weights\n", "            optimizer.step()\n", "            # print progress\n", "            bar.set_postfix(mse=float(loss))\n", "    # evaluate accuracy at end of each epoch\n", "    model.eval()\n", "    y_pred = model(X_test)\n", "    mse = loss_fn(y_pred, y_test)\n", "    mse = float(mse)\n", "    history.append(mse)\n", "    if mse < best_mse:\n", "        best_mse = mse\n", "        best_weights = copy.deepcopy(model.state_dict())\n", "\n", "# restore model and return best accuracy\n", "model.load_state_dict(best_weights)\n", "print(\"MSE: %.2f\" % best_mse)\n", "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n", "plt.plot(history)\n", "plt.show()\n", " \n", "model.eval()\n", "with torch.no_grad():\n", "    # Test out inference with 5 samples\n", "    for i in range(5):\n", "        X_sample = X_test_raw[i: i+1]\n", "        X_sample = scaler.transform(X_sample)\n", "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n", "        y_pred = model(X_sample)\n", "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"]}, {"cell_type": "markdown", "id": "050edac3", "metadata": {}, "source": ["Of course, there is still room to imporve the model. \n", "One way is to present the target in log scale or, equivalently, use mean absolute percentage error (MAPE) as the loss function. \n", "This is because the target variable is the value of houses and it is in a wide range. \n", "For the same error magnitude, it is more an issue for low-valued houses. \n", "It is your exercise to modify the above code to produce a better prediction."]}, {"cell_type": "markdown", "id": "a5688ecb", "metadata": {}, "source": ["## Summary"]}, {"cell_type": "markdown", "id": "b2296278", "metadata": {}, "source": ["In this post, you discovered the use of PyTorch to build a regression model.\n", "You learned how you can work through a regression problem step-by-step with PyTorch, specifically:\n", "\n", "- How to load and prepare data for use in PyTorch\n", "- How to create neural network models and choose a loss function for regression\n", "- How to improve model accuracy by applying standard scaler"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}}, "nbformat": 4, "nbformat_minor": 5}