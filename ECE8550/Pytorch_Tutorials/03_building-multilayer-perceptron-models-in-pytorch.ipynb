{"cells": [{"cell_type": "markdown", "id": "47db63ce", "metadata": {}, "source": ["# Building Multilayer Perceptron Models in PyTorch"]}, {"cell_type": "markdown", "id": "77404acd", "metadata": {}, "source": ["The PyTorch library is for deep learning. \n", "Deep learning, indeed, is just another name for a large scale neural network or multilayer perceptron network. \n", "In its simplest form, multilayer perceptrons are a sequence of layers connected in tandem. \n", "In this tutorial, you will discover the simple components you can use to create neural networks and simple deep learning models in PyTorch."]}, {"cell_type": "markdown", "id": "a95ebcc4", "metadata": {}, "source": ["## Overview"]}, {"cell_type": "markdown", "id": "0d06e8a1", "metadata": {}, "source": ["This tutorial is in six parts. \n", "They are:"]}, {"cell_type": "markdown", "id": "dc844fb3", "metadata": {}, "source": ["- Neural Network Models in PyTorch\n", "- Model Inputs\n", "- Layers, Activations, and Layer Properties\n", "- Loss Functions and Model Optimizers\n", "- Model Training and Inference\n", "- Examination of a Model"]}, {"cell_type": "markdown", "id": "df0edf1e", "metadata": {}, "source": ["## Neural Network Models in PyTorch"]}, {"cell_type": "markdown", "id": "edc63184", "metadata": {}, "source": ["PyTorch can do a lot of things but the most common use case is to build a deep learning model. \n", "The simplest model can be defined using Sequential class, which is just a linear stack of layers connected in tandem. \n", "You can create a Sequential model and define all the layers in one shot; for example:"]}, {"cell_type": "markdown", "id": "1578bcc9", "metadata": {}, "source": ["```python\n", "import torch\n", "import torch.nn as nn\n", "model = nn.Sequential(...)\n", "```"]}, {"cell_type": "markdown", "id": "d068ec89", "metadata": {}, "source": ["You should have all your layers defined inside the parentheses, in the processing order from input to output. \n", "For example,"]}, {"cell_type": "code", "execution_count": 1, "id": "f4d6d459", "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "model = nn.Sequential(\n", "    nn.Linear(764, 100),\n", "    nn.ReLU(),\n", "    nn.Linear(100, 50),\n", "    nn.ReLU(),\n", "    nn.Linear(50, 10),\n", "    nn.Sigmoid()\n", ")"]}, {"cell_type": "markdown", "id": "c995a562", "metadata": {}, "source": ["The other way of using Sequential is to pass in an ordered dictionary, such that we can assign names to each layer:"]}, {"cell_type": "code", "execution_count": 2, "id": "2bdc64bb", "metadata": {}, "outputs": [], "source": ["from collections import OrderedDict\n", "import torch.nn as nn\n", "\n", "model = nn.Sequential(OrderedDict([\n", "    ('dense1', nn.Linear(764, 100)),\n", "    ('act1', nn.ReLU()),\n", "    ('dense2', nn.Linear(100, 50)),\n", "    ('act2', nn.ReLU()),\n", "    ('output', nn.Linear(50, 10)),\n", "    ('outact', nn.Sigmoid()),\n", "]))"]}, {"cell_type": "markdown", "id": "af1cd6c7", "metadata": {}, "source": ["And if you would like to build the layers one by one instead of doing everything in one shot, you can do the following:"]}, {"cell_type": "code", "execution_count": 3, "id": "c2b3a1cf", "metadata": {}, "outputs": [], "source": ["model = nn.Sequential()\n", "model.add_module(\"dense1\", nn.Linear(8, 12))\n", "model.add_module(\"act1\", nn.ReLU())\n", "model.add_module(\"dense2\", nn.Linear(12, 8))\n", "model.add_module(\"act2\", nn.ReLU())\n", "model.add_module(\"output\", nn.Linear(8, 1))\n", "model.add_module(\"outact\", nn.Sigmoid())"]}, {"cell_type": "markdown", "id": "a1ef9b84", "metadata": {}, "source": ["You will find this helpful in a more complex case that you need to build a model based on some conditions."]}, {"cell_type": "markdown", "id": "b02b9e4f", "metadata": {}, "source": ["## Model Inputs"]}, {"cell_type": "markdown", "id": "d3be4cea", "metadata": {}, "source": ["The first layer in your model hints about the shape of the input. \n", "In the example above, we have nn.Linear(764, 100) as the first layer. \n", "Depends on the different layer type you use, the arguments may bear different meanings. \n", "But in this case, it is a Linear layer (also known as dense layer or fully connected layer) and the two arguments tells the input and output dimension of this layer."]}, {"cell_type": "markdown", "id": "a41e0d15", "metadata": {}, "source": ["Note that the size of a batch is implicit. \n", "In this example, we should pass in a PyTorch tensor of shape (n, 764) into this layer and expects a tensor of shape (n, 100) in return, which n is the size of a batch."]}, {"cell_type": "markdown", "id": "437fde8b", "metadata": {}, "source": ["## Layers, Activations, and Layer Properties"]}, {"cell_type": "markdown", "id": "d4f93bd7", "metadata": {}, "source": ["There are many kinds of neural network layers defined in PyTorch. \n", "In fact, it is easy to define your own layer if you want to. \n", "Below are some common layers that you may see often:"]}, {"cell_type": "markdown", "id": "9defe10e", "metadata": {}, "source": ["- `nn.Linear(input, output)`: The fully-connected layer\n", "- `nn.Conv2d(in_channel, out_channel, kernel_size)`: The 2D convolution layer, popular in image processing networks\n", "- `nn.Dropout(probability)`: Dropout layer, usually added to a network to introduce regularization\n", "- `nn.Flatten()`: Reshape a high-dimensional input tensor into 1-dimensional (per each sample in a batch)"]}, {"cell_type": "markdown", "id": "211cfba9", "metadata": {}, "source": ["Besides layers, there are also activation functions. \n", "They are functions applied to each element of a tensor. \n", "Usually we take the output of a layer and apply the activation before feeding it as input to a subsequent layer. \n", "Some common activation functions are:"]}, {"cell_type": "markdown", "id": "c8e4fa45", "metadata": {}, "source": ["- `nn.ReLU()`: Rectified linear unit, the most common activation nowadays\n", "- `nn.Sigmoid()` and `nn.Tanh()`: Sigmoid and hyperbolic tangent functions, which are the usually choice in older literatures\n", "- `nn.Softmax()`: To convert a vector into probability-like values. Popular in classification networks."]}, {"cell_type": "markdown", "id": "d9ab1653", "metadata": {}, "source": ["You can find a list of all different layers and activation functions in PyTorch's documentation."]}, {"cell_type": "markdown", "id": "b446ea9b", "metadata": {}, "source": ["The design of PyTorch is very modular. \n", "Therefore, you don't have much to adjust in each component. \n", "Take this Linear layer as an example, you can only specify the input and output shape but not other details such as how to initialize the weights. \n", "However, almost all components can take two additional arguments, the device and the data type."]}, {"cell_type": "markdown", "id": "6b0dbe33", "metadata": {}, "source": ["PyTorch device is to specify where will this layer execute. \n", "Normally you choose between the CPU and the GPU, or omit it and let PyTorch to decide. \n", "To specify a device, you do the following (CUDA means a supported nVidia GPU):"]}, {"cell_type": "code", "execution_count": 4, "id": "00b2be72", "metadata": {}, "outputs": [{"data": {"text/plain": ["Linear(in_features=764, out_features=100, bias=True)"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["nn.Linear(764, 100, device=\"cpu\")"]}, {"cell_type": "markdown", "id": "5923ca9d", "metadata": {}, "source": ["or"]}, {"cell_type": "code", "execution_count": 5, "id": "c7c0883f", "metadata": {}, "outputs": [], "source": ["if torch.cuda.is_available():\n", "    nn.Linear(764, 100, device=\"cuda:0\")"]}, {"cell_type": "markdown", "id": "77ae4349", "metadata": {}, "source": ["The data type argument (dtype) specifies what kind of data type this layer should operate on. \n", "Usually it is 32-bit float and usually you don't want to change that. \n", "But if you need to specify a different type, you must do so using PyTorch types, e.g.,"]}, {"cell_type": "code", "execution_count": 6, "id": "4f69a4c3", "metadata": {}, "outputs": [{"data": {"text/plain": ["Linear(in_features=764, out_features=100, bias=True)"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["nn.Linear(764, 100, dtype=torch.float16)"]}, {"cell_type": "markdown", "id": "f890727a", "metadata": {}, "source": ["## Loss Function and Model Optimizers"]}, {"cell_type": "markdown", "id": "f46bb4cc", "metadata": {}, "source": ["A neural network model is a sequence of matrix operations. \n", "The matrix that are independent of the input and keep inside the model are called weights. \n", "Training a neural network is to optimize these weights so that they produces the output we want. \n", "In deep learning, the algorithm to optimize these weights is gradient descent."]}, {"cell_type": "markdown", "id": "b4f9f9ce", "metadata": {}, "source": ["There are many variations of gradient descent. \n", "You can make your choice by preparing an optimizer for your model. \n", "It is not part of the model but you will use it alongside the model during training. \n", "The way you use it is to define a loss function, and minimize the loss function using the optimizer. \n", "Loss function is to give a distance score to tell how far away the model's output to your desired output. \n", "It compares the output tensor of the model to the expected tensor, which is called the label or the ground truth in different context. \n", "Because it is provided as part of the training dataset, neural network model is a supervised learning model."]}, {"cell_type": "markdown", "id": "898b6bdd", "metadata": {}, "source": ["In PyTorch, you can simply take the model's output tensor and manipulate it to calculate the loss. \n", "But you can also make use of the functions provided in PyTorch for that, e.g.,"]}, {"cell_type": "markdown", "id": "6f715e78", "metadata": {}, "source": ["```python\n", "loss_fn = nn.CrossEntropyLoss()\n", "loss = loss_fn(output, label)\n", "```"]}, {"cell_type": "markdown", "id": "6a6d631f", "metadata": {}, "source": ["In this example, the loss_fn is a function and loss is a tensor that supports automatic differentiation. \n", "You can trigger the differentiation by calling loss.backward()."]}, {"cell_type": "markdown", "id": "9ed25094", "metadata": {}, "source": ["Below are some commmon loss functions in PyTorch:"]}, {"cell_type": "markdown", "id": "6d22d09d", "metadata": {}, "source": ["- `nn.MSELoss()`: Mean square error, useful in regression problems\n", "- `nn.CrossEntropyLoss()`: Cross entropy loss, useful in classification problems\n", "- `nn.BCELoss()`: Binary cross entropy loss, useful in binary classification problems"]}, {"cell_type": "markdown", "id": "618263cd", "metadata": {}, "source": ["Creating an optimizer is similar:"]}, {"cell_type": "code", "execution_count": 7, "id": "e949e80a", "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]}, {"cell_type": "markdown", "id": "9a48b89e", "metadata": {}, "source": ["All optimizer would require a list of all parameters that it needs to optimize. \n", "It is because the optimizer is created outside of the model and you need to tell it where to look for the parameters (i.e., model weights). \n", "The optimizer will take the gradient as computed by the backward() function call and apply to the parameters based on the optimization algorithm."]}, {"cell_type": "markdown", "id": "4078147d", "metadata": {}, "source": ["These are a list of some common optimizers:"]}, {"cell_type": "markdown", "id": "c5957b92", "metadata": {}, "source": ["- `torch.optim.Adam()`: The Adam algorithm (adaptive moment estimation)\n", "- `torch.optim.NAdam()`: The Adam algorithm with Nesterov momentum\n", "- `torch.optim.SGD()`: Stochastic gradient descent\n", "- `torch.optim.RMSprop()`: The RMSprop algorithm"]}, {"cell_type": "markdown", "id": "eb5fd396", "metadata": {}, "source": ["You can find a list of all provided loss functions and optimizers in PyTorch's documentation. \n", "You can learn about the mathematical formula of each optimization algorithm in the respective optimizers' page in the documentation."]}, {"cell_type": "markdown", "id": "9fd37018", "metadata": {}, "source": ["## Model Training and Inference"]}, {"cell_type": "markdown", "id": "11accef3", "metadata": {}, "source": ["In PyTorch, you don't have a dedicated function for model training and evaluation. \n", "A defined model by itself, is like a function. \n", "You pass in an input tensor and get back the output tensor. \n", "Therefore, it is your responsibility to write the training loop. \n", "A minimal training loop is like the following:"]}, {"cell_type": "markdown", "id": "071ffa25", "metadata": {}, "source": ["```python\n", "for n in range(num_epochs):\n", "    y_pred = model(X)\n", "    loss = loss_fn(y_pred, y)\n", "    optimizer.zero_grad()\n", "    loss.backward()\n", "    optimizer.step()\n", "```"]}, {"cell_type": "markdown", "id": "86296c33", "metadata": {}, "source": ["If you already have a model, you can simply take y_pred = model(X) and use the output tensor y_pred for other purposes. \n", "That's how you use the model for prediction or inference. \n", "A model, however, do not expect one input sample, but a batch of input samples in one tensor. \n", "If the model is to take an input vector (which is one-dimensional), you should provide a two-dimensional tensor to the model. \n", "Usually in case of inference, we delibrately create a batch of one sample."]}, {"cell_type": "markdown", "id": "f98e540e", "metadata": {}, "source": ["## Examination of a Model"]}, {"cell_type": "markdown", "id": "4d635da3", "metadata": {}, "source": ["Once you have a model, you can check what is it by printing it:"]}, {"cell_type": "code", "execution_count": 8, "id": "e0b9924b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Sequential(\n", "  (dense1): Linear(in_features=8, out_features=12, bias=True)\n", "  (act1): ReLU()\n", "  (dense2): Linear(in_features=12, out_features=8, bias=True)\n", "  (act2): ReLU()\n", "  (output): Linear(in_features=8, out_features=1, bias=True)\n", "  (outact): Sigmoid()\n", ")\n"]}], "source": ["print(model)"]}, {"cell_type": "markdown", "id": "5a51757d", "metadata": {}, "source": ["If you would like to save the model, you can use the pickle library from Python. \n", "But you can also access it using PyTorch:"]}, {"cell_type": "code", "execution_count": 9, "id": "28c96fc3", "metadata": {}, "outputs": [], "source": ["torch.save(model, \"my_model.pickle\")"]}, {"cell_type": "markdown", "id": "6c4fbfe2", "metadata": {}, "source": ["This way, you have the entire model object saved in a pickle file. \n", "You can retrieve the model with"]}, {"cell_type": "code", "execution_count": 10, "id": "c6d94b13", "metadata": {}, "outputs": [], "source": ["model = torch.load(\"my_model.pickle\")"]}, {"cell_type": "markdown", "id": "16ca129d", "metadata": {}, "source": ["But the recommended way of saving a model is to leave the model design in code and save only the weights, you can do so with:"]}, {"cell_type": "code", "execution_count": 11, "id": "b21cedff", "metadata": {}, "outputs": [], "source": ["torch.save(model.state_dict(), \"my_model.pickle\")"]}, {"cell_type": "markdown", "id": "cf3ce1c5", "metadata": {}, "source": ["The state_dict() function extracts only the states (i.e., weights in a model). \n", "To retrieve it, you need to rebuild the model from scratch and then load the weights, like this:"]}, {"cell_type": "markdown", "id": "26f274b0", "metadata": {}, "source": ["```python\n", "model = nn.Sequential(...)\n", "model.load_state_dict(torch.load(\"my_model.pickle\"))\n", "```"]}, {"cell_type": "markdown", "id": "2aa5e247", "metadata": {}, "source": ["## Summary"]}, {"cell_type": "markdown", "id": "8bdff4c0", "metadata": {}, "source": ["In this tutorial, you discovered the PyTorch API that you can use to create artificial neural networks and deep learning models. \n", "Specifically, you learned about the life cycle of a PyToch model, including:"]}, {"cell_type": "markdown", "id": "500b5636", "metadata": {}, "source": ["- Constructing a model\n", "- Creating and adding layers and activations\n", "- Preparing model for training and inference"]}, {"cell_type": "markdown", "id": "3c556b9a", "metadata": {}, "source": ["## Resources"]}, {"cell_type": "markdown", "id": "9ff6f7cd", "metadata": {}, "source": ["You can learn more about how to create simple neural network and deep learning models in PyTorch using the following resources:"]}, {"cell_type": "markdown", "id": "0dd71ca3", "metadata": {}, "source": ["- [torch.nn documentation](https://pytorch.org/docs/stable/nn.html)\n", "- [torch.optim documentation](https://pytorch.org/docs/stable/optim.html)\n", "- [PyTorch tutorials](https://pytorch.org/tutorials/)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}}, "nbformat": 4, "nbformat_minor": 5}