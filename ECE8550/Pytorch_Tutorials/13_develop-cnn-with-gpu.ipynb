{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop CNN on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will study how to move DNN/CNN traning to GPU\n",
    "\n",
    "**To focus on the network architecture, we ignore the train/validation/test split, hyper-parameter tuning, and others.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "In this tutorial, you will discover how to develop a convolutional neural network using GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you can use the torchvision.transforms module to apply the normalization transformation to the train and test datasets. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           download=True,\n",
    "                                           transform=transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          download=True,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_dataset.transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "test_dataset.transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307, ), (0.3081, ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Deep FCN in GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the problem is a multi-class classification task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. \n",
    "This will also require the use of a softmax activation function. \n",
    "Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on CPU and on GPU are very similar.\n",
    "The only difference is to move your model and data to GPU.\n",
    "We first define `device` which is `gpu` if it is available.\n",
    "`cpu` is a backup in case `gpu` is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we move data and model to GPU using `.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: testing accuracy: 0.9317\n",
      "2: testing accuracy: 0.9492\n",
      "3: testing accuracy: 0.9600\n",
      "4: testing accuracy: 0.9675\n",
      "5: testing accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "dnn = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# move model to GPU\n",
    "dnn = dnn.to(device)\n",
    "\n",
    "lr = 0.1\n",
    "num_inputs, num_outputs = 784, 10\n",
    "optimizer = torch.optim.SGD(dnn.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):      \n",
    "        inputs, labels = sample_batched\n",
    "        # move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        prob_distr = dnn(inputs)\n",
    "        loss = criterion(prob_distr, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    accu_number = 0.\n",
    "    for X, y in test_dataloader:\n",
    "        # move test fata to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        inputs = X.reshape(-1, num_inputs)\n",
    "        predicted_class = torch.argmax(dnn(inputs), dim=1)\n",
    "        accu_number += torch.sum(predicted_class == y)\n",
    "    print(\n",
    "        f'{epoch+1}: testing accuracy: {accu_number / len(test_dataloader.dataset):0.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LeNet-5 on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will move **LeNet-5** to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: testing accuracy: 0.9658\n",
      "2: testing accuracy: 0.9728\n",
      "3: testing accuracy: 0.9801\n",
      "4: testing accuracy: 0.9848\n",
      "5: testing accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "lenet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1),\n",
    "    nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1),\n",
    "    nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.Flatten(start_dim=1,\n",
    "                                                       end_dim=-1),\n",
    "    nn.Linear(in_features=20 * 4 * 4, out_features=100), nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=10), nn.Softmax(dim=1))\n",
    "\n",
    "# move the model to gpu\n",
    "lenet = lenet.to(device)\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(lenet.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        inputs, labels = sample_batched\n",
    "        # move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        prob_distr = lenet(inputs)\n",
    "        loss = criterion(prob_distr, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    accu_number = 0.\n",
    "    for X, y in test_dataloader:\n",
    "        # move data to gpu\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        predicted_class = torch.argmax(lenet(X), dim=1)\n",
    "        accu_number += torch.sum(predicted_class == y)\n",
    "    print(\n",
    "        f'{epoch+1}: testing accuracy: {accu_number / len(test_dataloader.dataset):0.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to develop a convolutional neural network using GPU.\n",
    "The major benifit is speed. You can train a deep model in a very short time.\n",
    "In addtion, GPU has a large capacibility for parallel training."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
