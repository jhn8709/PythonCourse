{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a CNN for MNIST Handwritten Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST handwritten digit classification problem is a standard dataset used in computer vision and deep learning.\n",
    "Although the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch. \n",
    "This includes how to develop a robust test harness for estimating the performance of the model and how to explore improvements to the model.\n",
    "\n",
    "**To focus on the network architecture, we ignore the train/validation/test split, hyper-parameter tuning, and others.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "In this tutorial, you will discover how to develop a convolutional neural network for handwritten digit classification from scratch. \n",
    "After completing this tutorial, you will know:\n",
    "- How to develop a test harness to develop a robust evaluation of a model and establish a baseline of performance for a classification task.\n",
    "- How to explore extensions to a baseline model to improve learning and model capacity.\n",
    "- How to develop a CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "\n",
    "This tutorial is divided into five parts.\n",
    "- MNIST Handwritten Digit Classification Dataset\n",
    "- Model Evaluation Methodology\n",
    "- How to Develop a Baseline Model\n",
    "- How to Develop an Improved Model\n",
    "- How to Develop a CNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Handwritten Digit Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset.\n",
    "It is a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.\n",
    "The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.\n",
    "\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 60,000 examples in the training dataset and 10,000 in the test dataset and that images are indeed square with 28×28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Develop a Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to develop a baseline model.\n",
    "This is critical as it both involves developing the infrastructure for the test harness so that any model we design can be evaluated on the dataset, and it establishes a baseline in model performance on the problem, by which all improvements can be compared.\n",
    "The design of the test harness is modular, and we can develop a separate function for each piece. \n",
    "This allows a given aspect of the test harness to be modified or inter-changed, if we desire, separately from the rest.\n",
    "We can develop this test harness with five key elements. \n",
    "They are the loading of the dataset, the preparation of the dataset, the definition of the model, the evaluation of the model, and the presentation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you can use the torchvision.transforms module to apply the normalization transformation to the train and test datasets. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           download=True,\n",
    "                                           transform=transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          download=True,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_dataset.transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "test_dataset.transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307, ), (0.3081, ))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a dataset, there are a lot of data sample or instances. \n",
    "You can ask the model to take one sample at a time but usually you would let the model to process one batch of several samples. \n",
    "You may create a batch by extracting a slice from the dataset, using the slicing syntax on the tensor. \n",
    "For a better quality of training, you may also want to shuffle the entire dataset on each epoch so no two batch would be the same in the entire training loop. \n",
    "Sometimes, you may introduce data augmentation to manually introduce more variance to the data. \n",
    "This is common for image-related tasks, which you can randomly tilt or zoom the image a bit to generate a lot of data sample from a few images.\n",
    "You can imagine there can be a lot of code to write to do all these. \n",
    "But it is much easier with the **DataLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a FCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a MLP with a SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: testing accuracy: 0.9129\n",
      "2: testing accuracy: 0.9201\n",
      "3: testing accuracy: 0.9255\n",
      "4: testing accuracy: 0.9265\n",
      "5: testing accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "mlp = nn.Sequential(nn.Linear(784, 10), nn.Softmax(dim=1))\n",
    "\n",
    "lr = 0.1\n",
    "num_inputs, num_outputs = 784, 10\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        inputs, labels = sample_batched\n",
    "        inputs = inputs.reshape(-1, num_inputs)\n",
    "        prob_distr = mlp(inputs)\n",
    "        loss = criterion(prob_distr, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    accu_number = 0.\n",
    "    for X, y in test_dataloader:\n",
    "        inputs = X.reshape(-1, num_inputs)\n",
    "        predicted_class = torch.argmax(mlp(inputs), dim=1)\n",
    "        accu_number += torch.sum(predicted_class == y)\n",
    "    print(\n",
    "        f'{epoch+1}: testing accuracy: {accu_number / len(test_dataloader.dataset):0.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a traditional shadow model with low accuracy.\n",
    "One may attempt to improve it by adding more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Deep FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two main aspects: the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the problem is a multi-class classification task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. \n",
    "This will also require the use of a softmax activation function. \n",
    "Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: testing accuracy: 0.9334\n",
      "2: testing accuracy: 0.9484\n",
      "3: testing accuracy: 0.9619\n",
      "4: testing accuracy: 0.9682\n",
      "5: testing accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "dnn = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "lr = 0.1\n",
    "num_inputs, num_outputs = 784, 10\n",
    "optimizer = torch.optim.SGD(dnn.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        inputs, labels = sample_batched\n",
    "        prob_distr = dnn(inputs)\n",
    "        loss = criterion(prob_distr, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    accu_number = 0.\n",
    "    for X, y in test_dataloader:\n",
    "        inputs = X.reshape(-1, num_inputs)\n",
    "        predicted_class = torch.argmax(dnn(inputs), dim=1)\n",
    "        accu_number += torch.sum(predicted_class == y)\n",
    "    print(\n",
    "        f'{epoch+1}: testing accuracy: {accu_number / len(test_dataloader.dataset):0.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more layers, i.e., a deeper model, the performance is (slightly) improved.\n",
    "But it is very slow to train as there are so many parameters.\n",
    "In addition, it is hard to train as there may be more unexpectation.\n",
    "Compared with the effort, the improvement is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Develop an Improved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to change the model configuration in order to explore improvements over the baseline models.\n",
    "We will implement the very famous **LeNet-5** by adding convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: testing accuracy: 0.9596\n",
      "2: testing accuracy: 0.9694\n",
      "3: testing accuracy: 0.9782\n",
      "4: testing accuracy: 0.9831\n",
      "5: testing accuracy: 0.9871\n"
     ]
    }
   ],
   "source": [
    "lenet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1),\n",
    "    nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1),\n",
    "    nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.Flatten(start_dim=1,\n",
    "                                                       end_dim=-1),\n",
    "    nn.Linear(in_features=20 * 4 * 4, out_features=100), nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=10), nn.Softmax(dim=1))\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(lenet.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        inputs, labels = sample_batched\n",
    "        prob_distr = lenet(inputs)\n",
    "        loss = criterion(prob_distr, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    accu_number = 0.\n",
    "    for X, y in test_dataloader:\n",
    "        predicted_class = torch.argmax(lenet(X), dim=1)\n",
    "        accu_number += torch.sum(predicted_class == y)\n",
    "    print(\n",
    "        f'{epoch+1}: testing accuracy: {accu_number / len(test_dataloader.dataset):0.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to develop a convolutional neural network for handwritten digit classification from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to develop a test harness to develop a robust evaluation of a model and establish a baseline of performance for a classification task.\n",
    "- How to explore extensions to a baseline model to improve learning and model capacity.\n",
    "- How to develop a CNN model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
