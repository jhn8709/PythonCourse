{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a PyTorch Model with DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build and train a PyTorch deep learning model, you can provide the training data in several different ways. \n",
    "Ultimately, a PyTorch model works like a function that takes a PyTorch `tensor` and returns you another `tensor`. \n",
    "You have a lot of freedom in how to get the input tensors. \n",
    "Probably the easiest is to prepare a large tensor of the entire dataset and extract a small batch from it in each training step. \n",
    "But you will see that using the `DataLoader` can save you a few lines of code in dealing with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "In this tutorial, you will see how you can use the the Data and DataLoader in PyTorch. \n",
    "After finishing this tutorial, you will learn:\n",
    "\n",
    "- How to create and use DataLoader to train your PyTorch model\n",
    "- How to use Data class to generate data on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is DataLoader?\n",
    "- Using DataLoader in a Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a deep learning model, you need data. \n",
    "Usually data is available as a dataset. \n",
    "In a dataset, there are a lot of data sample or instances. \n",
    "You can ask the model to take **one sample** at a time but usually you would let the model to process one batch of several samples. \n",
    "You may create **a batch** by extracting a slice from the dataset, using the slicing syntax on the tensor. \n",
    "For a better quality of training, you may also want to **shuffle** the entire dataset on each epoch so no two batch would be the same in the entire training loop. \n",
    "Sometimes, you may introduce **data augmentation** to manually introduce more variance to the data. \n",
    "This is common for image-related tasks, which you can randomly tilt or zoom the image a bit to generate a lot of data sample from a few images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine there can be a lot of code to write to do all these. \n",
    "But it is much easier with the `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of how create a `DataLoader` and take a batch from it. \n",
    "In this example, the [sonar dataset](https://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) is used and ultimately, it is converted into PyTorch tensors and passed on to DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read data, convert to NumPy arrays\n",
    "data = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "    header=None)\n",
    "X = data.iloc[:, 0:60].values\n",
    "y = data.iloc[:, 60].values\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "# convert into PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# create DataLoader, then take one batch\n",
    "loader = DataLoader(list(zip(X, y)), shuffle=True, batch_size=16)\n",
    "for X_batch, y_batch in loader:\n",
    "    print(X_batch, y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the output of above that `X_batch` and `y_batch` are PyTorch tensors. \n",
    "The loader is an instance of `DataLoader` class which can work like an iterable. \n",
    "Each time you read from it, you get a batch of features and targets from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a `DataLoader` instance, you need to provide a list of sample pairs. \n",
    "Each sample pair is one data sample of feature and the corresponding target. \n",
    "A list is required because DataLoader expect to use `len()` to find the total size of the dataset and using array index to retrieve a particular sample. \n",
    "The batch size is a parameter to `DataLoader` so it knows how to create a batch from the entire dataset. \n",
    "You should almost always use `shuffle=True` so every time you load the data, the samples are shuffled. \n",
    "It is useful for training because in each epoch, you are going to read every batch once. \n",
    "When you proceed from one epoch to another, as `DataLoader` knows you depleted all the batches, it will re-shuffle so you get a new combination of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataLoader in a Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example to make use of DataLoader in a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train-test split for evaluation of the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    train_size=0.7,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# set up DataLoader for training set\n",
    "loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)\n",
    "\n",
    "# create model\n",
    "model = nn.Sequential(nn.Linear(60, 60), nn.ReLU(), nn.Linear(60, 30),\n",
    "                      nn.ReLU(), nn.Linear(30, 1), nn.Sigmoid())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 200\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# evaluate accuracy after training\n",
    "model.eval()\n",
    "y_pred = model(X_test)\n",
    "acc = (y_pred.round() == y_test).float().mean()\n",
    "acc = float(acc)\n",
    "print(\"Model accuracy: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that once you created the `DataLoader` instance, the training loop can only be easier. \n",
    "In the above, only the training set is packaged with a DataLoader because you need to loop through it in batches. \n",
    "You can also create a `DataLoader` for the test set and use it for model evaluation, but since the accuracy is computed over the entire test set rather than in a batch, the benefit of DataLoader is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together, below is the complete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data, convert to NumPy arrays\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\", header=None)\n",
    "X = data.iloc[:, 0:60].values\n",
    "y = data.iloc[:, 60].values\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "# convert into PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train-test split for evaluation of the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    train_size=0.7,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# set up DataLoader for training set\n",
    "loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)\n",
    "\n",
    "# create model\n",
    "model = nn.Sequential(nn.Linear(60, 60), nn.ReLU(), nn.Linear(60, 30),\n",
    "                      nn.ReLU(), nn.Linear(30, 1), nn.Sigmoid())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 200\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# evaluate accuracy after training\n",
    "model.eval()\n",
    "y_pred = model(X_test)\n",
    "acc = (y_pred.round() == y_test).float().mean()\n",
    "acc = float(acc)\n",
    "print(\"Model accuracy: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Iterator using Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there is a `Dataset` class that can be tightly coupled with the `DataLoader` class. \n",
    "Recall that `DataLoader` expects its first argument can work with `len()` and with array index. \n",
    "The `Dataset` class is a base class for this. \n",
    "The reason you may want to use `Dataset` class is there are **some special handling before you can get the data sample**. \n",
    "For example, data should be read from database or disk and you only want to keep a few samples in memory rather than prefetch everything. \n",
    "Another example is to perform real-time preprocessing of data, such as random augmentation that is common in image tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `Dataset` class, you just subclass from it and implement two member functions. \n",
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # convert into PyTorch tensors and remember them\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        features = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the most powerful way to use Dataset but simple enough to demonstrate how it works. \n",
    "With this, you can create a `DataLoader` and use it for model training. \n",
    "Modifying from the previous example, you have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up DataLoader for training set\n",
    "dataset = SonarDataset(X_train, y_train)\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "# create model\n",
    "model = nn.Sequential(nn.Linear(60, 60), nn.ReLU(), nn.Linear(60, 30),\n",
    "                      nn.ReLU(), nn.Linear(30, 1), nn.Sigmoid())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 200\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy after training\n",
    "model.eval()\n",
    "y_pred = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "acc = (y_pred.round() == y_test).float().mean()\n",
    "acc = float(acc)\n",
    "print(\"Model accuracy: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You set up dataset as an instance of `SonarDataset` which you implemented the `__len__()` and `__getitem__()` functions. \n",
    "This is used in place of the list in the previous example to set up the `DataLoader` instance. \n",
    "Afterward, everything is the same in the training loop. \n",
    "Note that you still use PyTorch tensors directly for the test set in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `__getitem__()` function, you take an integer that works like an array index and returns a pair, the features and the target. \n",
    "You can implement anything in this function: run some code to generate a synthetic data sample, read data on the fly from the internet, or add random variations to the data. \n",
    "You will also find it useful in the situation that you cannot keep the entire dataset in memory, so you can load only the data samples that you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since you created a PyTorch dataset, you don't need to use scikit-learn to split data into training set and test set. \n",
    "In `torch.utils.data` submodule, you have a function `random_split()` that works with Dataset class for the same purpose. \n",
    "A full example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, default_collate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read data, convert to NumPy arrays\n",
    "data = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "    header=None)\n",
    "X = data.iloc[:, 0:60].values\n",
    "y = data.iloc[:, 60].values\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y).reshape(-1, 1)\n",
    "\n",
    "\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # convert into PyTorch tensors and remember them\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        features = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return features, target\n",
    "\n",
    "\n",
    "# set up DataLoader for data set\n",
    "dataset = SonarDataset(X, y)\n",
    "trainset, testset = random_split(dataset, [0.7, 0.3])\n",
    "loader = DataLoader(trainset, shuffle=True, batch_size=16)\n",
    "\n",
    "# create model\n",
    "model = nn.Sequential(nn.Linear(60, 60), nn.ReLU(), nn.Linear(60, 30),\n",
    "                      nn.ReLU(), nn.Linear(30, 1), nn.Sigmoid())\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 200\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# create one test tensor from the testset\n",
    "X_test, y_test = default_collate(testset)\n",
    "model.eval()\n",
    "y_pred = model(X_test)\n",
    "acc = (y_pred.round() == y_test).float().mean()\n",
    "acc = float(acc)\n",
    "print(\"Model accuracy: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very similar to the example you have before. \n",
    "Beware that the PyTorch model still needs a tensor as input, not a Dataset. \n",
    "Hence in the above, you need to use the `default_collate()` function to collect samples from a dataset into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you learned how to use DataLoader to create shuffled batches of data and how to use Dataset to provide data samples. \n",
    "Specifically you learned:\n",
    "\n",
    "- DataLoader as a convenient way of providing batches of data to the training loop\n",
    "- How to use Dataset to produce data samples\n",
    "- How combine Dataset and DataLoader to generate batches of data on the fly for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Readings\n",
    "\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "- [torch.utils.data](https://pytorch.org/docs/stable/data.html) from PyTorch documentation\n",
    "- [Datasets and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) from PyTorch tutorial"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
